- Real world optimization problems often have multiple objectives and are constrained with limitations like size and weight for example in car design. The analysis done in the paper used unconstrained single-objective optimisation problems to reduce the complexity. We don't know how this analysis would hold up for multi-objective prpblems and no exlanation or approach has been mentioned regarding how they plan on solving this for multi-objective problems.
- One of things to note is, if an algorithm passes all test fucntions in BBOB or any other classical benchmark test suites, that doesn't guarantee that it would perform as effectively or efficiently for a real world problem.[https://www2.pcs.usp.br/~jaime/papers/carvalho_optmas18_e.pdf] In the same manner, we don't know how effective the artifical fucntions generated would perform in a real world scenario that is even if we can optimistically assume that these functions in future can be scaled to be used for multi-objective and constrained problems.
- One of the severe limitations with research experiment they are conducting is the small amount of data available for the experiment. Although they are bootstrapping to overcome this issue, it doesn't give a high confidence with regards to the effectiveness of the results.
- Out of the 1000 artificial functions they generated, a few are found to be closely clustered with the problem instances and also there are a few problem instances as they mentioned such as D4_P1 which doesn't have close clusters for Rotation function, that shows a limitation with these artificial functions. And it can be assumed that this limitation might increase with the increase in sample size and multiple objectives.
- Another issue that can be thought of is, it's mentioned in the paper that there are often limitations with the function evaluation budget. It's not explained how this could effect the creation and evaluation of artificial functions as more than 1000 functions might be requried to evaluate some problem instances like D4_P1.
- The clustering approach that they used for comparision of computations is okay in this instance as they argue that it is still applicable for the problems they chose, but it's highly unlikely that this approach would hold up when the sample size is high and more complexity is introduced. There isn't any explanation given on how they would like to tackle this problem or any other alternative approaches.
- It's mentioned in the paper that ELA features are sensitive to sample size, meaning different sample sizes produce varying results. A study claims that, some features actually provide robust measures with low sample size of 50 x D (D = 10) and 100 x D, but the sample size needed for generating robust ELA measures across all features is somewhere around 800 x D, and that it is sufficient enough to robust measures for 95% of benchmark problems. The sample size chosen in this experiment is much low compared to that and we don't know how the computations will differ for higher sample sizes. [https://www.mdpi.com/1999-4893/14/3/78/htm]